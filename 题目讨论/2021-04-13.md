
#背景
以稳定性体系建设作为指标管控

##故障分类

###代码编写

```全局map
//全局data 权限-
data.Region = map[string][]user2.RegionCityInfo{}
//不断被刷


2.panic 情况、go fun (set 数据。然后访问同一块指针。导致挂掉) .加锁问题 、Troubleshooting

3.嵌套多个 goroutine 忘记 recover

4. go 并发 使用少。导致读写冲突、比如 并发读写map
```


###资源不足
```
1.内存泄漏  https://segmentfault.com/a/1190000020103403 go oom 问题。原因是map 每次都new 没有被释放

2. 系统文件数耗尽   redis 每次都new 实例，导致tcp 端口被大量占用。引发file 句柄无法持有 发生panic

3.常见的 Goroutine 泄露所导致的 CPU、Memory 上涨等，还是得看你的 Goroutine 里具体在跑什么东西。
```




###系统设计问题

```
1. 性能开销问题
for range 一个3K+的 大map.每次都去range. QPS 过高的时候 会发生CPUIO 飙升。接口响应超时
因为range 会进行值拷贝、时间损耗在拷贝、要40+us.经过压测实践、占用cpu高。性能变低、当QPS 1000的时候。就额外开销40ms. 而 我们的QPS 1000是1000ms..当加剧的时候。我们的性能会变低、会发生抢占CPU的情况、造成开销过大

2.大量go 启动 导致 无法回收、重启大法
```


###配置问题
```
1.runtime 获取核心数问题为什么获取到错误的 cpu 数，会导致业务耗时增长这么严重？主要还是对延迟要求太敏感了，然后又是高频服务，runtime 的影响被放大了。
关于怎么解决获取正确核数的问题，目前 golang 服务的解决方式主要是两个，第一是设置环境变量 GOMAXPROCS 启动，第二是显式调用 uber/automaxprocs。
golang 是如何设置 cpu 核数，并取成宿主的核数的呢，追 runtime.NumCPU() 发现，其实现细追，发现是 getproccount, 查 linux 下源码发现，其实调用的是 sched_getaffinity，直接通过系统调用获取的宿主机核数。
https://mp.weixin.qq.com/s?__biz=MzU1ODEzNjI2NA==&mid=2247487164&idx=2&sn=594a9fee9f54660cf3b12a3ce6a2ea88&source=41#wechat_redirect

numcpu 是否一定和P一样就好 我们可以trace 看到多少个P。线程创建数 不会超过他4-5倍吧

```



